{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª LAB-Bench: Evaluate AI on Biology Research Tasks\n",
    "\n",
    "This notebook evaluates AI models on neurology and biology research capabilities:\n",
    "- Literature analysis (LitQA)\n",
    "- Database retrieval (DbQA)\n",
    "- Figure interpretation (FigQA)\n",
    "- Table analysis (TableQA)\n",
    "- Protocol troubleshooting (ProtocolQA)\n",
    "\n",
    "**Use Case for Neurologists:**\n",
    "- Benchmark AI assistants for your research needs\n",
    "- Test models on neurology-specific questions\n",
    "- Compare different AI models for clinical applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone LAB-Bench repository\n",
    "!git clone https://github.com/Future-House/LAB-Bench.git\n",
    "%cd LAB-Bench\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -e .\n",
    "!pip install -q datasets pillow\n",
    "\n",
    "print(\"âœ… LAB-Bench installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Setup API key for LLM\n",
    "try:\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… OpenAI API key loaded\")\n",
    "except:\n",
    "    try:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "        print(\"âœ… Anthropic API key loaded\")\n",
    "    except:\n",
    "        print(\"âš ï¸ No API key found. Add OPENAI_API_KEY or ANTHROPIC_API_KEY to secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load LAB-Bench Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ðŸ“¥ Loading LAB-Bench dataset from Hugging Face...\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"futurehouse/lab-bench\", split=\"train\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(dataset)} questions\")\n",
    "print(f\"\\nðŸ“Š Dataset Structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Convert to pandas for easier exploration\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Categories Available:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” SAMPLE QUESTIONS FROM LAB-BENCH\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show examples from different categories\n",
    "categories_to_show = ['LitQA2', 'FigQA', 'ProtocolQA']\n",
    "\n",
    "for category in categories_to_show:\n",
    "    category_data = df[df['category'] == category]\n",
    "    if len(category_data) > 0:\n",
    "        sample = category_data.iloc[0]\n",
    "        print(f\"\\nðŸ“Œ Category: {category}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Question: {sample['question']}\")\n",
    "        print(f\"\\nAnswer Choices:\")\n",
    "        for choice_key in ['choice_A', 'choice_B', 'choice_C', 'choice_D']:\n",
    "            if choice_key in sample and pd.notna(sample[choice_key]):\n",
    "                print(f\"  {choice_key[-1]}: {sample[choice_key]}\")\n",
    "        print(f\"\\nCorrect Answer: {sample.get('correct_answer', 'N/A')}\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Simple AI Agent for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "async def simple_agent(question, choices, image=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Simple agent that uses GPT-4o-mini to answer multiple choice questions.\n",
    "    \n",
    "    Args:\n",
    "        question: The question text\n",
    "        choices: Dict with keys A, B, C, D containing answer choices\n",
    "        image: Optional PIL image for visual questions\n",
    "    \n",
    "    Returns:\n",
    "        str: Single letter answer (A, B, C, or D)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format choices\n",
    "    choices_text = \"\\n\".join([f\"{k}: {v}\" for k, v in choices.items() if v])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"You are an expert in neurology and biological sciences.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer Choices:\n",
    "{choices_text}\n",
    "\n",
    "Please provide ONLY the letter (A, B, C, or D) of the correct answer.\n",
    "\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip().upper()\n",
    "    \n",
    "    # Extract just the letter\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        if letter in answer:\n",
    "            return letter\n",
    "    \n",
    "    return 'A'  # Default fallback\n",
    "\n",
    "print(\"âœ… Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Evaluation on Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"ðŸ§ª RUNNING EVALUATION\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on a small subset (10 questions)\n",
    "sample_size = 10\n",
    "sample_data = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for idx, row in sample_data.iterrows():\n",
    "    # Prepare choices\n",
    "    choices = {\n",
    "        'A': row.get('choice_A', ''),\n",
    "        'B': row.get('choice_B', ''),\n",
    "        'C': row.get('choice_C', ''),\n",
    "        'D': row.get('choice_D', '')\n",
    "    }\n",
    "    \n",
    "    # Get agent's answer\n",
    "    predicted = await simple_agent(row['question'], choices)\n",
    "    \n",
    "    # Check correctness\n",
    "    correct_answer = row.get('correct_answer', 'N/A')\n",
    "    is_correct = (predicted == correct_answer)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    results.append({\n",
    "        'Category': row['category'],\n",
    "        'Question': row['question'][:50] + '...',\n",
    "        'Predicted': predicted,\n",
    "        'Correct': correct_answer,\n",
    "        'Match': 'âœ“' if is_correct else 'âœ—'\n",
    "    })\n",
    "    \n",
    "    print(f\"Question {len(results)}/{sample_size}: {row['category']} - {'âœ“' if is_correct else 'âœ—'}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (correct / len(results)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nðŸ“Š EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuestions Evaluated: {len(results)}\")\n",
    "print(f\"Correct Answers: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "# Show detailed results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\nðŸ“‹ Detailed Results:\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Category-wise Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate per-category accuracy\n",
    "category_performance = results_df.groupby('Category').apply(\n",
    "    lambda x: (x['Predicted'] == x['Correct']).sum() / len(x) * 100\n",
    ").reset_index()\n",
    "category_performance.columns = ['Category', 'Accuracy (%)']\n",
    "\n",
    "print(\"ðŸ“Š CATEGORY-WISE PERFORMANCE\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(category_performance.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "if len(category_performance) > 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=category_performance, x='Category', y='Accuracy (%)')\n",
    "    plt.title('AI Performance by LAB-Bench Category')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.axhline(y=25, color='red', linestyle='--', label='Random Chance (25%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create results directory\n",
    "results_dir = '/content/drive/MyDrive/LAB_Bench_Results'\n",
    "!mkdir -p \"{results_dir}\"\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_df.to_csv(f\"{results_dir}/evaluation_results_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save summary report\n",
    "report_file = f\"{results_dir}/evaluation_summary_{timestamp}.txt\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"LAB-BENCH EVALUATION REPORT\\n\")\n",
    "    f.write(f\"Generated: {datetime.datetime.now()}\\n\")\n",
    "    f.write(f\"Model: GPT-4o-mini\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Overall Accuracy: {accuracy:.1f}%\\n\")\n",
    "    f.write(f\"Questions Evaluated: {len(results)}\\n\\n\")\n",
    "    f.write(\"Category Performance:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(category_performance.to_string(index=False))\n",
    "\n",
    "print(f\"âœ… Results saved to Google Drive:\")\n",
    "print(f\"   {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Next Steps\n",
    "\n",
    "### For Neurology Research:\n",
    "\n",
    "**1. Test on More Questions:**\n",
    "Increase `sample_size` to 50-100 for more robust evaluation\n",
    "\n",
    "**2. Filter for Neurology-Specific Questions:**\n",
    "```python\n",
    "# Filter dataset for neuroscience/neurology topics\n",
    "neuro_keywords = ['neuron', 'brain', 'cognitive', 'alzheimer', 'parkinson', 'epilepsy']\n",
    "neuro_questions = df[df['question'].str.lower().str.contains('|'.join(neuro_keywords))]\n",
    "```\n",
    "\n",
    "**3. Compare Different Models:**\n",
    "Test multiple models:\n",
    "- GPT-4o (more powerful, more expensive)\n",
    "- Claude-3.5-Sonnet (Anthropic)\n",
    "- GPT-4o-mini (faster, cheaper)\n",
    "\n",
    "**4. Add Domain Knowledge:**\n",
    "Enhance agent with neurology-specific context:\n",
    "```python\n",
    "prompt = f\"\"\"You are a board-certified neurologist with expertise in \n",
    "neurodegenerative diseases, neuroimaging, and clinical neurology.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**5. Integration with Clinical Practice:**\n",
    "- Benchmark AI assistants before using in clinical research\n",
    "- Test on your own neurology quiz questions\n",
    "- Evaluate for specific tasks (e.g., image interpretation, protocol design)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
